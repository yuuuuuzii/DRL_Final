/home/chunyulin/anaconda3/envs/offline_rl/lib/python3.10/site-packages/gymnasium/envs/registration.py:519: DeprecationWarning: [33mWARN: The environment HalfCheetah-v4 is out of date. You should consider upgrading to version `v5`.[0m
  logger.deprecation(
  2%|â–ˆâ–‰                                                                                                                       | 16/1000 [03:51<3:56:53, 14.44s/it]
Episode 0: ELoss: 11037.22, Recon Loss: 10.58, SAC Loss: 0.47, Reward: -435.72
Episode 1: ELoss: 15399.18, Recon Loss: 14.76, SAC Loss: 0.66, Reward: -358.90
Episode 2: ELoss: 15485.87, Recon Loss: 14.91, SAC Loss: 0.59, Reward: -367.03
Episode 3: ELoss: 15428.32, Recon Loss: 14.80, SAC Loss: 0.64, Reward: -333.92
Episode 4: ELoss: 15287.74, Recon Loss: 14.69, SAC Loss: 0.61, Reward: -345.19
Episode 5: ELoss: 15219.33, Recon Loss: 14.56, SAC Loss: 0.68, Reward: -321.47
Episode 6: ELoss: 15098.47, Recon Loss: 14.44, SAC Loss: 0.67, Reward: -311.20
Episode 7: ELoss: 14947.99, Recon Loss: 14.27, SAC Loss: 0.70, Reward: -313.59
Episode 8: ELoss: 14816.73, Recon Loss: 14.12, SAC Loss: 0.71, Reward: -312.25
Episode 9: ELoss: 14709.58, Recon Loss: 13.98, SAC Loss: 0.74, Reward: -332.85
Episode 10: ELoss: 14639.85, Recon Loss: 13.90, SAC Loss: 0.75, Reward: -323.20
Episode 11: ELoss: 14599.92, Recon Loss: 13.85, SAC Loss: 0.76, Reward: -311.14
Episode 12: ELoss: 14497.92, Recon Loss: 13.75, SAC Loss: 0.77, Reward: -315.93
Episode 13: ELoss: 14372.57, Recon Loss: 13.62, SAC Loss: 0.77, Reward: -326.78
Episode 14: ELoss: 14234.53, Recon Loss: 13.46, SAC Loss: 0.79, Reward: -322.12
Episode 15: ELoss: 14105.11, Recon Loss: 13.29, SAC Loss: 0.83, Reward: -311.49
Traceback (most recent call last):
  File "/home/chunyulin/Desktop/DRL/DRL_Final/train.py", line 121, in <module>
    main()
  File "/home/chunyulin/Desktop/DRL/DRL_Final/train.py", line 76, in main
    sac_loss, enc_dec_loss = agent.update_actor_critic(agent.replay_buffer, batch_size)
  File "/home/chunyulin/Desktop/DRL/DRL_Final/utils.py", line 238, in update_actor_critic
    action_pi, log_pi = self.actor.sample(states)
  File "/home/chunyulin/Desktop/DRL/DRL_Final/utils.py", line 117, in sample
    mu, std = self.forward(state)
  File "/home/chunyulin/Desktop/DRL/DRL_Final/utils.py", line 109, in forward
    x = self.net(state)
  File "/home/chunyulin/anaconda3/envs/offline_rl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/chunyulin/anaconda3/envs/offline_rl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/chunyulin/anaconda3/envs/offline_rl/lib/python3.10/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/home/chunyulin/anaconda3/envs/offline_rl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/chunyulin/anaconda3/envs/offline_rl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/chunyulin/anaconda3/envs/offline_rl/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt
